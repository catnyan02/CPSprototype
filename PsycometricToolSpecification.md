

Conceptual assessment framework of  
CONSTRUCT  
Problem-Solving Ability

Authors:  
Niloofar Rezaei  
Anastasiia Nesterenko

Date  
01.12.2025

Table of Contents

[General Information about the Instrument	3](#general-information-about-the-instrument)

[Content Measured by the Instrument	7](#content-measured-by-the-instrument)

[Content-activity matrix	11](#content-activity-matrix)

[References	28](#references)

# General Information about the Instrument {#general-information-about-the-instrument}

1. **Purpose and Measurement Object**

The instrument is designed to measure complex problem-solving ability in undergraduate students and it assesses how learners understand, explore, and control unfamiliar dynamic systems using academic-life microworlds.

The broader psychological construct is problem-solving ability in complex and dynamic tasks. In line with general models of problem solving, it is understood as a person‚Äôs capacity to understand a problem and construct an internal representation of the situation, generate and evaluate solution strategies, implement and adapt these strategies to achieve the goal of solving the problem. This view is consistent with classical theories that emphasize goal representation, planning, monitoring and metacognition, and evaluation of outcomes (e.g., D√∂rner‚Äôs work on complex problem solving; Mayer‚Äôs problem-solving processes; Polya‚Äôs stages of understanding, devising, carrying out, and reflecting on a plan).

The instrument is grounded in the MicroDYN framework (Greiff & Funke, 2009), which conceptualizes problem solving as interaction with small dynamic systems whose structure is initially unknown to the solver. Our primary purpose is to evaluate: Knowledge acquisition (discovering causal relations) (Greiff & Funke, 2009; Krieger et al., 2021), Knowledge application (using this structure to reach target goals) (Schweizer et al., 2013\) and Strategic processes (exploration style, systematic and random behaviour) (Stadler et al., 2019).This aligns with higher-education needs, where students frequently face ill-structured and dynamic academic tasks requiring planning, adaptation, and reflective decision-making (Chernikova et al., 2020).

In addition to this, it is important to underline that the construct of problem-solving is broader than the MicroDYN methodology. General frameworks of problem-solving (for example D√∂rner‚Äôs theory of complex problem solving, Mayer‚Äôs problem-solving processes, or Polya‚Äôs stages) also include components such as goal representation, planning, metacognition, emotional regulation, creativity, and long-term strategic reasoning (D√∂rner, 1997; Mayer, 1992; Polya, 1945). MicroDYN captures only a subset of the full construct, specifically structural knowledge acquisition, control performance, and short-term exploration strategies (Greiff & Funke, 2009; Moln√°r & Csap√≥, 2018). Therefore, the structure of our instrument reflects the aspects of problem-solving that MicroDYN is able to operationalize, while acknowledging that other theoretical components of problem-solving are outside the scope of this tool.

To ensure alignment between the construct and the instrument, our assessment focuses on the CPS components that MicroDYN can validly operationalize. From the broader construct of problem solving (which includes goal representation, planning, metacognition, emotional regulation, etc.), MicroDYN directly measures two central cognitive processes‚Äîknowledge acquisition and knowledge application‚Äîand provides logfile-based indicators of exploration strategy use. Therefore, our instrument targets the CPS components that MicroDYN is empirically validated to assess, while acknowledging that other components of the construct lie outside the measurement scope.

It is also important to explain why this structure was selected over alternative models. MicroDYN provides a validated and widely used operationalization of dynamic problem solving in educational contexts, and its two-phase architecture (exploration and control) maps clearly onto measurable behaviors (Greiff et al., 2013; OECD, 2014). At the same time, other problem-solving frameworks highlight additional domains, but these cannot be captured with the present methodology (Funke, 2014). 

MicroDYN is selected as the assessment framework because it provides a validated, efficient, and theoretically grounded method for measuring key components of complex problem solving (CPS). One of its major advantages is the clear separation of knowledge acquisition and knowledge application, which allows researchers to measure how students construct causal models and how they use these models to reach goals. This two-phase structure has been repeatedly supported in empirical research (Greiff et al., 2013; Herrmann et al., 2023). MicroDYN also demonstrates strong psychometric validity and reliability: causal-model scoring produces stable factors and predicts academic performance beyond reasoning and working memory (Greiff et al., 2013), and its structure yields interpretable and reliable CPS scores across diverse populations (Alrababah et al., 2024). A further advantage is its capacity to analyze process data, including systematic exploration strategies such as VOTAT, which are strongly associated with successful CPS performance and can be reliably captured in logfiles (Wu & Moln√°r, 2021). Compared with classical microworlds like Tailorshop or GeneticsLab, which offer realism but often suffer from low reliability, strong dependence on prior knowledge, and difficulty isolating latent constructs (Schoppek & Fischer, 2015), MicroDYN was intentionally designed as a psychometrically sound operationalization of specific CPS components. It enables standardized, low-verbal-load, and scalable assessment within short administration times and has been validated in multiple cultural contexts, including Europe, Asia, and the Middle East (Alrababah et al., 2024; Wu & Moln√°r, 2021). Although it does not capture all aspects of real-world CPS, MicroDYN provides a robust, cross-culturally feasible, and theoretically aligned method for assessing core cognitive processes in undergraduate students. For these reasons, MicroDYN is used as the primary instrument in this assessment.

Although the instrument structure presents example items for a single microworld, the full assessment consists of three parallel microworlds. Each microworld includes the same set of item types (exploration, causal diagram, control tasks, and process-data indicators), but with different system parameters. This results in 24 scored items total (8 items per microworld). This multi-microworld structure follows recommendations in the MicroDYN literature, where repeated tasks increase reliability, reduce task-specific variance, and stabilize causal-model and control scores (Greiff et al., 2012; Moln√°r & Csap√≥, 2018). The conceptual definitions remain constant across microworlds, while Instrument Structure illustrates the item formats using one microworld for clarity.

2. **Inferences and Decisions Based on Test Results**

Based on the test results, we can make inferences about a student‚Äôs ability to analyze unfamiliar systems and extract causal structure (knowledge acquisition), plan and adjust strategies to reach conflicting goals (knowledge application),use efficient strategies such as VOTAT (Stadler et al., 2019\) and monitor and adjust behaviour in dynamic conditions (Krieger et al., 2021).

These results may help instructors or researchers identify:

* Students who rely on ineffective exploration strategies.

* Students who struggle to plan and regulate actions in dynamic tasks.

* Strengths and weaknesses in academic problem-solving processes.

Results should not be used for grading or selection; the assessment is designed for diagnostic and pedagogical purposes only.

In addition, inferences must be interpreted with caution because MicroDYN captures only specific aspects of problem-solving and does not measure motivational factors, emotional regulation, collaborative problem solving, or long-term planning. Therefore, decisions based on the test results apply only to the domains that MicroDYN assesses.

3. **Target Testing Population**

The instrument is intended for undergraduate university students enrolled in bachelor‚Äôs programmes. This population is justified because higher education students are expected to develop complex skills such as problem solving, adaptation and managing uncertainty (Chernikova et al., 2020). Undergraduates frequently face ill-structured tasks, conflicting deadlines and ambiguous instructions (conditions that match Complex Problem Solving (CPS) task demands). Complex microworld tasks have been successfully used with undergraduate samples in CPS research (Greiff & Funke, 2009; Schweizer et al., 2013\) and Students are expected to have: basic computer literacy (at least), and at least one year of higher-education experience (recommended, but not required).

4. **Key Testing Limitations**

The key testing limitations are as follows:

* Requires basic digital literacy.

* Linear microworlds cannot represent all real-life academic complexity.

* Motivation, fatigue, and unfamiliarity with simulations may affect performance.

* Process indicators (such as VOTAT) may vary in reliability unless clearly defined (Stadler et al., 2019).

* Scores reflect performance in simulated academic scenarios, which may not generalize to real world tasks.

* In addition to these limitations, it is important to note the methodological limitations of MicroDYN itself. MicroDYN systems are typically linear, have a limited number of variables, and do not include emotional, motivational, or social elements of problem solving. MicroDYN also does not cover long-term planning, creativity, transfer of learning, or metacognitive reflection. Therefore, the assessment captures a restricted but well-defined subset of the broader construct of problem solving.

5. **Instrument Validity Evidence**

Our instrument is strongly supported by empirical CPS research.

**Structure Validity and Theoretical Basis:**

MicroDYN tasks capture both rule identification and rule application reliably (Greiff & Funke, 2009\) and CPS requires discovering unknown system structures and applying them (exactly what the instrument measures) (Krieger et al., 2021).

Additional evidence for construct validity comes from studies showing that MicroDYN scores relate meaningfully to external cognitive measures. For example, rule knowledge and rule application remain significantly correlated even after controlling for working memory, indicating that MicroDYN captures CPS-specific processes rather than general cognitive load (Schweizer et al., 2013). The study also demonstrates incremental validity, as MicroDYN rule knowledge predicts school grades beyond working memory capacity, showing that it measures unique cognitive processes relevant for academic performance. Furthermore, the use of multiple items and repeated control rounds increases validity by reducing prior-knowledge effects and allowing learners to update their internal models. Together, these results support MicroDYN as a psychometrically valid tool for assessing key components of complex problem solving.

**Process Validity:**

Systematic strategies like VOTAT predict higher structural knowledge (Stadler et al., 2019\) and microworld tasks capture meaningful strategy patterns relevant for problem solving.

**Ecological Validity:**

Simulation-based learning research demonstrates that higher-education tasks are often dynamic, uncertain, and require multi-goal coordination (precisely the environment microworld tasks simulate) (Chernikova et al., 2020). So the instrument has a strong theoretical and empirical foundation for assessing CPS in undergraduate students.

In addition to this, reliability considerations are important for interpreting test results. MicroDYN-based assessments typically demonstrate acceptable reliability for knowledge acquisition and knowledge application scores, but reliability of strategy-use indicators can vary depending on the number of trials and consistency of participant behavior. In the future version of the instrument, reliability will be examined using internal consistency estimates and stability of log-data indicators across tasks.

**Future Validation Plans:**

Planned procedures include item-level psychometric analysis, evaluation of convergent and discriminant validity with related constructs, and model-based scoring validation (for example: process data modeling).

# Content Measured by the Instrument {#content-measured-by-the-instrument}

Problem-solving ability in dynamic contexts includes how individuals understand, explore, and control unfamiliar systems in order to reach goals. Research on MicroDYN-type tasks consistently shows that problem solving includes two core cognitive dimensions and a third strategic behavioral dimension. These three dimensions are strongly supported in the scientific literature: Knowledge Acquisition (Exploration), which refers to MicroDYN Phase 1 (identifying causal relations), Knowledge Application (Control) referring to MicroDYN Phase 2 (using knowledge to reach target states), and Strategy Use (Process Indicators) refers to log-data‚Äìbased analysis. (quality of exploration behavior, e.g., VOTAT)

However, it is important to separate the universal construct of problem-solving from the MicroDYN methodology. General problem-solving models (for example D√∂rner‚Äôs theory of complex problem solving, Mayer‚Äôs problem-solving processes, and Polya‚Äôs stages) include broader components such as goal representation, planning, metacognition, emotional regulation, and creative reasoning. MicroDYN captures only a limited subset of this broader construct, specifically structural knowledge acquisition, short-term control performance, and observable trial-based exploration strategies. Therefore, the dimensions presented below reflect the aspects of problem-solving that can be operationalized with MicroDYN and do not represent the whole range of processes involved in human problem-solving.

This structure is explicitly documented in Moln√°r & Csap√≥ (2018), where exploration (Phase 1), control (Phase 2), and strategy patterns (VOTAT and non-VOTAT) are treated as distinct analytical components of problem solving. This justification also explains why these three dimensions were selected instead of alternative models: they correspond directly to behaviors that MicroDYN can measure reliably.

Below, we provide a detailed discussion of each dimension and its indicators, together with the supporting literature, and summarize them in Table 1\.

**Dimension 1: Knowledge Acquisition (Exploration)**

This dimension reflects a student‚Äôs ability to discover the underlying causal structure of an unfamiliar dynamic system during the exploration phase. 

Indicators we use in our instrument:

* Identifying correct causal relations between variables  
* Correct identification of polarity between the causal relations  
* Accurate representation of the causal model (causal diagram) with properly identified magnitudes

It is important to note that this dimension represents only the structural-knowledge aspect of problem-solving. Other components of knowledge acquisition described in broader frameworks, such as conceptual understanding or hypothesis generation, are not captured by MicroDYN and therefore not included here.

As such, the indicators we chose are directly supported by MicroDYN research:

* Correct identification of causal relations between inputs and outputs;  
  (MicroDYN Phase 1 focuses on detecting which input influences which output; Schweizer et al., 2013; Moln√°r & Csap√≥, 2018\)

* Correct identification of polarity between the causal relations  
  (MicroDYN systems are formalized as linear structural equations, where coefficients can be positive or negative; correct rule knowledge thus inherently includes correct sign of the coefficients; Funke, J., & Greiff, S., 2017\)

* Accurate representation of the causal model (causal diagram) with properly identified magnitudes  
  (Participants produce a causal diagram showing the direction and presence/absence of effects; Schweizer et al., 2013\)

**Dimension 2: Knowledge Application (Control)**

This dimension reflects how well a student can apply the discovered causal relations to reach target states in the control phase.

Indicators:

*  Reaching target values within allowed steps  
*  Correct and efficient adjustment of input variables  
*  Minimal errors, overshooting, and unnecessary corrections

It is important to note that this dimension corresponds to short-term control behavior in a dynamic system. It does not include other kinds of application processes described in broader literature, such as long-term planning, transfer of knowledge, or evaluating alternative solutions, which cannot be measured with the present methodology.

As such, the indicators we chose are directly supported by MicroDYN scoring principles:

* Achieving target output values within the allowed number of steps;  
  (Control performance is evaluated based on accuracy of reaching the goal state; Schweizer et al., 2013\)

* Adjusting input variables in a way that is consistent with the identified causal relations;  
  (Knowledge application requires using the correct input configuration to influence outputs; Moln√°r & Csap√≥, 2018\)

* Avoiding inefficient patterns such as overshooting, undershooting, or unnecessary corrections.  
  (Process indicators include overshooting/undershooting patterns and adjustment efficiency; Stadler et al., 2019; Moln√°r & Csap√≥, 2018\)

**Dimension 3: Strategy Use (Process Indicators)**

This dimension reflects the strategic behaviour students employ while interacting with the system, especially during exploration.

Indicators:

*  Use of systematic exploration strategies (e.g., VOTAT)  
*  Use of structured exploration sequences (ordered, non-random exploration)

It is important to note that these indicators refer specifically to the patterns observable in MicroDYN log data and should not be interpreted as the full spectrum of metacognitive or strategic processes described in general problem-solving theory. More complex strategic abilities such as monitoring, planning, or regulating motivation are not assessed here.

As such, the indicators we chose are directly supported by VOTAT and process-data literature referenced in MicroDYN and follow-up studies:

* Use of systematic exploration strategies such as VOTAT (varying one input at a time);  
  (High-quality VOTAT is identified as an efficient exploration strategy; Stadler et al., 2019; Moln√°r & Csap√≥, 2018\)

* Use of structured exploration sequences, where inputs are explored one-by-one in a consistent order rather than through random switching.   
  (Structured exploration has been shown to produce clearer system states and support the discovery of causal relations Stadler et al., 2019; Moln√°r & Csap√≥, 2018).

It is important to note that these strategy indicators belong specifically to the MicroDYN approach. They do not represent all strategic processes described in the broader problem-solving literature, such as metacognitive planning or motivational regulation.

Table 1\. Problem Solving Dimensions: Definitions and Indicators

| Dimension | Definition (Based on Sources) | Indicators (Supported by Sources) |
| ----- | ----- | ----- |
| 1\. Knowledge Acquisition (Exploration) | Ability to discover the causal structure of an unfamiliar dynamic system during the exploration phase. (MicroDYN Phase 1; Moln√°r & Csap√≥, 2018; Schweizer et al., 2013\) This definition reflects the part of problem-solving that MicroDYN can operationalize, while broader aspects of knowledge acquisition from general problem-solving models are not included here. | ‚Ä¢ Correct identification of causal relations between inputs and outputs  ‚Ä¢ Correct identification of polarity between the causal relations ‚Ä¢ Accurate representation of the causal model (causal diagram) with properly identified magnitudes |
| 2\. Knowledge Application (Control) | Ability to use the discovered causal relations to reach target output values in the control phase. (MicroDYN Phase 2; Schweizer et al., 2013; Moln√°r & Csap√≥, 2018\) This refers only to short-term system control as captured in MicroDYN and does not include other forms of problem-solving application such as long-term planning or evaluating alternatives | ‚Ä¢ Achieving target output values within allotted steps  ‚Ä¢ Adjusting inputs in line with the identified causal rules ‚Ä¢ Avoidance of inefficiencies (overshooting, undershooting, unnecessary corrections) |
| 3\. Strategy Use (Process Indicators) | Strategic behaviours displayed during exploration, especially systematic vs. unsystematic trial behaviour. (Stadler et al., 2019; Moln√°r & Csap√≥, 2018). These strategies are specific to what can be observed in MicroDYN log data and do not represent the full range of strategic and metacognitive processes described in broader problem-solving theory. | ‚Ä¢ Use of systematic exploration strategies (VOTAT) ‚Ä¢ Use of structured exploration sequences, where inputs are explored one-by-one in a consistent order rather than through random switching. |

# Content-activity matrix {#content-activity-matrix}

The assessment consists of three microworlds. Each microworld is defined by the state equation:

*Yt*  \= *A*  *Yt* \+ *B*  *Xt* \+ *C*

Where:

* *Yt* represents the vector of output variables at time *t*.

* *Xt* represents the vector of input variables at time *t*.

* *A* is the transition matrix representing the system's eigendynamics (how the system changes itself over time).

* *B* is the effect matrix representing the causal influence of inputs on outputs.

* *C* is a constant vector (optional).

In our instrument we simplify the system to focus on the B matrix (direct effects), usually with A0  to reduce the difficulty.

Each microworld task contributes:

* 3 acquisition items

* 3 application items

* 2 strategy-use items

Total \= 24 items, balanced across all dimensions. This structure matches the MicroDYN task architecture described in Moln√°r & Csap√≥ (2018) and Schweizer et al. (2013). 

It is important to note that the blueprint has two levels: a conceptual level defining the three dimensions of problem-solving, and an instrumental level showing how these dimensions are implemented through the MicroDYN tasks. The selection of item types reflects the processes that MicroDYN can measure reliably (for example causal identification, control behavior, and observable strategies), while other components of problem-solving from broader models are not included because they cannot be operationalized with this methodology.

The overall blueprint of the instrument is summarized in Table 2\. It presents the three assessed dimensions ‚Äì Knowledge Acquisition, Knowledge Application, and Strategy Use ‚Äì along with the corresponding item types and the number of items allocated to each dimension.

Table 2\. Test Blueprint: Dimensions, Item Types, and Number of Items

| Measured Dimension | Item Types | Items |
| ----- | ----- | ----- |
| Knowledge Acquisition | Causal diagram, polarity classification, magnitude rating | 9  (3 per each microworld) |
| Knowledge Application | Microworld control, input-selection items | 9  (3 per each microworld) |
| Strategy Use | VOTAT indicator, exploration-sequence analysis | 6  (2 per each microworld) |
| TOTAL |  | 24  (8 per each microworld) |

These item types correspond directly to the indicators defined previously. For example, the causal-diagram item targets the representation indicator, while the VOTAT indicator targets systematic strategy use. This alignment ensures that each indicator has at least one item format capable of capturing it.

At the same time, it should be noted that these item types measure only those aspects of problem-solving that MicroDYN tasks can capture. Other forms of reasoning, planning or metacognition described in general problem-solving theories cannot be represented through these formats.

Building on the overall test blueprint, Table 3 provides an item-level specification of the instrument. For each dimension, it lists the corresponding indicators, activity/item types, concrete task descriptions, and the number of items targeting each indicator. This structure explains both the conceptual mapping (dimension ‚Üí indicator) and the instrumental implementation (indicator ‚Üí item type). The indicators represent the theoretical dimensions of CPS, while the item types show how these indicators can be operationalized within the MicroDYN environment. The choice of item formats is based on what MicroDYN can measure reliably (for example changes in inputs, trial sequences, causal-diagram accuracy).

Table 3\. Item-Level Blueprint: CPS Dimensions, Indicators, Activities, and Item Counts

| Dimension | Indicator | Activity / Item Type | Item Description | Item Count |
| :---- | :---- | :---- | :---- | :---- |
| Knowledge Acquisition[^1] (Exploration) | Identifying causal relations (Topology) | Causal- diagram structure item | Student completes a causal diagram by drawing arrows from each input variable to the output variable(s) they believe it directly influences. | 3 (1 per each microworld) |
|  | Identifying effect polarity (Positive/ Negative) | Polarity- classification item | For each drawn causal arrow, the student indicates whether the effect is positive (increase‚Äìincrease / decrease‚Äìdecrease) or negative (increase‚Äìdecrease / decrease‚Äìincrease). | 3 (1 per each microworld) |
|  | Identifying effect magnitude  (1 \= Weak 2 \= Moderate 3 \= Strong) | Magnitude- rating item | For each causal arrow, the student rates the strength of the effect on a 3-point scale (1 \= weak, 2 \= moderate, 3 \= strong). | 3 (1 per each microworld) |
| Subtotal: |  |  |  | 9 items |
| 2\. Knowledge Application[^2] (Control) | Reaching target values | Microworld ‚ÄúControl Phase‚Äù item | Student adjusts inputs to move the system toward target output states within limited steps. | 3 (1 per each microworld) |
|  | Correct input adjustments | Input- selection item | Student chooses appropriate input values consistent with identified causal model. | 3 (1 per each microworld) |
|  | Avoiding overshoot / undershoot | Input- selection item | The student selects the option that best describes the most efficient input-adjustment behaviour. Inefficient behaviour includes overshooting (increasing an input too strongly and needing to reverse it) and undershooting (making adjustments that are too small to reach the goal). | 3 (1 per each microworld) |
| Subtotal: |  |  |  | 9 items |
| 3\. Strategy Use[^3] (Process Indicators) | Use of VOTAT strategy | Log-data strategy indicator | System evaluates whether a student varies one input at a time during exploration. | 3 (1 per each microworld) |
|  | Systematic Exploration Sequence | Log-data strategy indicator | System evaluates whether a student explores inputs one-by-one in a structured order, instead of random switching. | 3 (1 per each microworld) |
| Subtotal: |  |  |  | 6 items |
| TOTAL |  |  |  | 24 ITEMS |

These items represent only the aspects of the CPS construct that can be operationalized in a MicroDYN environment. Other important components of problem-solving such as long-term planning, collaborative reasoning, emotional regulation, or creative solution generation are not assessed by these item formats.

**1\. Instrument Structure and Item Examples**

Here, we present example items for a single microworld ‚ÄúSemester Manager System‚Äù, the full assessment consists of three parallel microworlds. The microworlds follow the standard linear system structure used in MicroDYN tasks, represented by the state equation *Yt*  \= *A*  *Yt* \+ *B*  *Xt* \+ *C*. Each microworld includes the same set of item types (exploration, causal diagram, control, strategy-use indicators), but with different system parameters. This results in a total of 24 scored items (8 per microworld). This multi‚Äìmicroworld structure follows recommendations in the MicroDYN literature since repeated items increase reliability and reduce task-specific variance.

**Microworld Context: Semester Manager System**

Students interact with a simulated system with three controllable input variables (Study Hours, Group Work Effort, Self-Care/Rest) and three observable outputs (Course Performance, Stress Level, Project Progress). The relationships between variables are not shown to students and must be discovered through interaction. The relationships are described by the effect matrix *B* representing the causal influence of inputs on outputs. Constant vector *C* and the transition matrix *A* representing the system's eigendynamics from the state equation are not utilized in this microworld.

Before presenting the items, it is important to note that the following tasks represent the instrumental implementation of the CPS construct. They capture only the aspects of knowledge acquisition, knowledge application, and strategy use that can be measured within the MicroDYN environment. Broader processes described in general problem-solving theories (for example metacognition, long-term planning, emotional regulation) are not included here because they cannot be operationalized with this methodology.

**Dimension 1: Knowledge Acquisition (3 items)**

These items operationalize the conceptual process of knowledge acquisition as defined in Step 2\. They measure how students extract causal structure from a dynamic system. They represent only the part of knowledge acquisition that MicroDYN can assess and do not cover broader processes such as hypothesis generation or complex reasoning.

Item 1: Identifying Causal Relations (Topology)

Item type: Causal-diagram structure item

Instructions: Students are presented with the three input variables (Study Hours, Group Work Effort, Self-Care/Rest) and the three output variables (Course Performance, Stress Level, Project Progress). Based on their exploration, students draw arrows from each input to the output variable(s) that they believe it directly influences.

Scoring:

Score \= max(0, TP ‚àí FP)

Where:

* TP \= number of correctly drawn arrows (true causal links drawn)

* FP \= number of incorrect arrows (arrows drawn where no causal link exists)

Rationale:

This item measures the student‚Äôs ability to identify the existence of causal links (topology) in the system. The scoring procedure rewards correct identification of causal links (topology) while explicitly penalising incorrect arrows, thereby reducing false positives due to over-connection strategies.

Item 2: Identifying Effect Polarity (Positive/Negative)

Item type: Polarity-classification item

Instructions: For every arrow the student drew in Item KA1, they must now indicate whether the effect is positive (an increase in the input leads to an increase in the output, or a decrease leads to a decrease) or negative (an increase in the input leads to a decrease in the output, or vice versa). Students assign a ‚Äú+‚Äù or ‚Äú‚Äì‚Äù sign to each causal arrow.

Scoring:

1 point for each correctly identified polarity.

0 points for incorrect polarity assignments.

Rationale:

This item measures students‚Äô understanding of the direction of causal influence, a core component of knowledge acquisition in MicroDYN.

Item 3: Identifying Effect Magnitude (Strength of Influence)

Item type: Magnitude-rating item

Instructions: For each causal arrow identified in Item KA1, students assign a numerical strength value based on the size of the observed effect during exploration. Students use a 3-point scale:

1 \= weak influence

2 \= moderate influence

3 \= strong influence

Students enter the appropriate number beside each arrow.

Scoring:

Each correctly identified magnitude value \= 1 point.

Magnitude is considered correct when the student‚Äôs response matches the true effect size embedded in the microworld.

Rationale:

This item assesses the student's ability to quantify causal strength, completing the causal-model representation required for accurate system understanding.

**Dimension 2: Knowledge Application (3 items)**

The following items represent the instrumental implementation of knowledge application. They capture short-term control behaviour based on discovered causal relations but do not measure broader decision-making or planning processes described in other problem-solving frameworks.

Item 4: Control Task (Reaching Target State)

Item type: Microworld control

Instructions: Students adjust inputs to reach the following target outputs within six steps: Course Performance ‚â• 80, Stress Level ‚â§ 40, Project Progress ‚â• 60\.

Scoring: 2 \= all targets reached; 1 \= some targets reached; 0 \= no targets reached.

This item reflects real-time system control and corresponds to the indicator reaching target values.

Item 5: Choosing Appropriate Input Adjustments

Item type: Multiple-choice

Instructions: Students choose the best input adjustment for increasing Course Performance while minimizing Stress.

Options:

 A. Increase Study Hours a lot

 B. Increase Study Hours slightly and increase Rest

 C. Increase Group Work only

 D. Decrease Rest and increase Study Hours significantly

Correct answer: B

Scoring: 1 for correct response, 0 otherwise.

Rationale:

A multiple-choice format is used because it measures decision-making accuracy according to the identified causal model.

Item 6: Avoiding overshoot / undershoot 

Item type: Multiple-choice 

Instructions: Students choose the best final slider adjustment based on the causal model discovered in the exploration phase. The correct option yields an additional \+6 points in Project Progress while keeping Stress at or below its current value (no overshoot in Stress and no undershoot in Progress).

Options:

 A. Increase Study Hours by 2 points and Group Work by 2 points; leave Rest unchanged.

 B. Keep Study Hours and Group Work unchanged; increase Rest by 3 points.

 C. Increase Group Work by 3 points and Rest by 3 points; leave Study Hours unchanged.

 D. Increase Study Hours by 3 points and Rest by 2 points; leave Group Work unchanged.

Correct answer:C

Scoring: 1 for correct response, 0 otherwise.

Rationale:

A multiple-choice format is used because it measures decision-making accuracy according to the identified causal model.

**Dimension 3: Strategy Use (2 items)**

These items operationalize strategy use as observable behaviour in MicroDYN log data. They measure systematic vs unsystematic exploration patterns but do not represent the full range of strategic or metacognitive processes.

Item 7: VOTAT Strategy Indicator

Item type: Log-data process indicator

Instructions: During exploration, the system records whether the student varies only one input at a time.

Scoring: 1 \= at least three valid VOTAT trials; 0 \= fewer than three.

Rationale:

Log-data indicators were selected because strategy use is best captured through actual trial sequences.

Item 8: Systematic Exploration Sequence

Item type: Log-data process indicator

Instructions: During the exploration system records logs and then uses these logs to classify the student‚Äôs exploration sequence as systematic if each input is tested individually at least once in a non-random order.

Scoring:

1 \= systematic (e.g., Study Hours only ‚Üí Group Work only ‚Üí Self-Care only)

0 \= random or chaotic sequence

Rationale:

Log-data indicators were selected because strategy use is best captured through actual trial sequences.

**Order of presentation**

Across the full assessment, students work through three microworlds that all use the same sequence of activities. The microworlds are presented one after another in a fixed order to all students. Within each microworld, the order of tasks is identical to preserve comparability and to avoid contamination across phases (e.g., control tasks never precede causal-diagram items).

1\. Start of microworld: Exploration phase

For each microworld (e.g., Semester Manager System), students first see only the interactive control panel with the three input sliders (Study Hours, Group Work Effort, Self-Care/Rest) and the three outputs (Course Performance, Stress Level, Project Progress).

At this stage, there are no explicit items on screen; students are simply instructed to explore how the system works.

The system records all slider adjustments and resulting output changes as log data. These log files provide the basis for the Strategy Use indicators. Thus, Dimension 3: Strategy Use is captured concurrently with exploration and does not appear as separate, visible items for students.

Only after the exploration phase is finished (when the student chooses to continue), the system moves to the explicit knowledge-acquisition items.

2\. Knowledge Acquisition items (Dimension 1; Items 1‚Äì3)

Immediately after exploration in the same microworld, students complete three knowledge-acquisition items in the following order:

Item 1 ‚Äì Identifying Causal Relations (Topology)

Students draw arrows from each input (Study Hours, Group Work Effort, Self-Care/Rest) to the output(s) they believe it directly influences (Course Performance, Stress Level, Project Progress).

Item 2 ‚Äì Identifying Effect Polarity (Positive/Negative)

For every arrow they drew in Item 1, students now assign a ‚Äú+‚Äù or ‚Äú‚Äì‚Äù sign indicating whether the effect is positive or negative.

Item 3 ‚Äì Identifying Effect Magnitude (Strength of Influence)

For each causal arrow identified in Item 1, students rate the strength of the influence on a 3-point scale (1 \= weak, 2 \= moderate, 3 \= strong).

This order reflects the logic of causal-model building:

first identifying which variables are connected (topology), then the direction of influence (polarity), and finally the size of the effect (magnitude).

3\. Knowledge Application items (Dimension 2; Items 4‚Äì6)

After completing the causal-diagram items, students proceed to the knowledge-application tasks in the same microworld. Here, the causal model they have built is applied to concrete control demands.

Item 4 ‚Äì Control Task (Reaching Target State)

Students again interact with the microworld and are asked to reach specific target values (Course Performance ‚â• 80, Stress Level ‚â§ 40, Project Progress ‚â• 60\) within a fixed number of steps.

Item 5 ‚Äì Choosing Appropriate Input Adjustments (Multiple-choice)

Students select the best input adjustment to increase Course Performance while minimizing Stress from four given options (A‚ÄìD).

Item 6 ‚Äì Avoiding Overshoot / Undershoot (Multiple-choice)

Students choose the best final input adjustment that yields an additional \+6 points in Project Progress while keeping Stress at or below its current value (no overshoot in Stress, no undershoot in Progress) from four given options (A‚ÄìD).

4\. Transition to the next microworld

Once Items 1‚Äì6 for the current microworld are completed:

The microworld is closed and cannot be revisited.

The system stores all product scores (Items 1‚Äì6) and the log-data based strategy-use scores (Items 7‚Äì8 and any additional indicator) for that microworld.

Students then proceed to the next microworld, which follows exactly the same order of presentation. This structure is repeated for all three microworlds, resulting in a total of 24 scored items (8 per microworld: 3 knowledge acquisition, 3 knowledge application, 2 strategy-use indicators), all embedded in a consistent and theoretically grounded order of presentation.

**2\. Key Instrument Features**

MicroDYN tasks share several structural and procedural features that support reliable assessment of complex problem solving. Each task uses a standardized linear input/output system with clearly defined causal dependencies, allowing consistent comparison across microworlds (Greiff et al., 2012). Task demands are structured into two phases :exploration and control,which enables direct assessment of knowledge acquisition and knowledge application (W√ºstenberg et al., 2012). Similar to computer-based CPS tasks used in PISA, MicroDYN runs on a computer interface requiring precise manipulation of sliders and buttons; this setup supports accurate capture of real-time interactions and minimizes the influence of prior knowledge by using decontextualized, fictitious systems.

The instrument also incorporates a detailed process-data design. All user actions (e.g., slider movements, application of changes, sequence of trials) are automatically logged, enabling analysis of strategic behaviors such as VOTAT. Such logfile-based indicators have been shown to reflect meaningful differences in CPS performance in large-scale assessments (OECD, 2014; Greiff et al., 2015). Finally, MicroDYN‚Äôs multi-task structure and repeated rounds contribute to higher reliability by providing multiple opportunities for participants to demonstrate rule identification, system control, and strategy use. These design features collectively allow the instrument to capture specific cognitive and behavioral components of CPS within a short, standardized, computer-based environment.

**3\. Scoring system** 

The instrument yields 24 scored items, organised into three parallel microworlds (8 items per microworld). Within each microworld, items are grouped into three dimensions:

* Knowledge Acquisition (3 items)

* Knowledge Application (3 items)

* Strategy Use (Process Indicators) (2 items, of which two are specified below)

All items are first scored at the microworld level. Dimension scores are then obtained by aggregating across the three microworlds.

For clarity, we denote raw score on item ùëñ in microworld ùëö as ùëÜùëñ,ùëö where ùëö ‚àà {1, 2, 3} is a microworld index.

**Item-level scoring**

*Knowledge Acquisition (Items 1‚Äì3)*

All Knowledge Acquisition items are scored based on the student‚Äôs causal diagram and parameter annotations.

Let:

* Etrue= set of true causal links in the effect matrix B

* ‚à£Etrue‚à£=L= number of true causal links

* Edrawn= set of arrows drawn by the student

* TP=‚à£EdrawnEtrue‚à£= number of correctly drawn links

* FP=‚à£Edrawn‚àñEtrue‚à£= number of spurious links

Item 1 (Topology): Identifying Causal Relations

Scoring rule (per microworld):

S1,m=max‚Å°(0,TP-FP)

Range: 0‚â§S1,m‚â§L

Interpretation: Correct links increase the score; each incorrect arrow cancels one correct arrow, which discourages over-connection strategies that would inflate TP by drawing many arrows. Scores are not allowed to become negative, which encourages exploration

Item 2 (Polarity): Identifying Effect Polarity (+/‚Äì)  
For each arrow the student drew in Item 1, they assign a ‚Äú+‚Äù or ‚Äú‚Äì‚Äù.

An arrow‚Äôs polarity is correct if:

1. The arrow corresponds to a true causal link in Etrue, and

   2. The sign matches the true effect direction in the underlying system.

Arrows that do not correspond to a true link (i.e., are false positives) are automatically counted as incorrect for polarity.

Scoring rule (per microworld):

S2,m=number of arrows with correctly identified polarity

Range: 0‚â§S2,m‚â§L

Item 3 (Magnitude): Identifying Effect Magnitude (1‚Äì3)  
For each causal arrow the student drew, they assign a magnitude on a 3-point scale (1 \= weak, 2 \= moderate, 3 \= strong).

A magnitude rating is correct if:

1. The arrow corresponds to a true causal link, and

   2. The magnitude matches the true effect size parameter embedded in the microworld.

Scoring rule (per microworld):

S3,m=number of arrows with correctly identified magnitude

Range: 0‚â§S3,m‚â§L

(For false-positive links, magnitude is always counted as incorrect.)

*Knowledge Application (Items 4‚Äì6)*

Item 4: Control Task (Reaching Target State)  
Students adjust the three inputs during a six-step control phase to reach:

* Course Performance ‚â• 80

* Stress Level ‚â§ 40

* Project Progress ‚â• 60

The final outputs after the last allowed control step (or when the student confirms the final state) are evaluated against these criteria.

Scoring rule (per microworld):

* S4,m=2 if all three target conditions are met

* S4,m=1 if at least one but not all targets are met

* S4,m=0 if none of the target conditions are met

Range: 0‚Äì2

Item 5: Choosing Appropriate Input Adjustments  
Multiple-choice item with four options (A‚ÄìD); the correct response is B.

Scoring rule (per microworld):

* S5,m=1 if the student selects option B

* S5,m=0 otherwise

Range: 0‚Äì1

Item 6: Avoiding Overshoot / Undershoot  
Multiple-choice item; the correct response is C (final slider adjustment that yields \+6 Project Progress while keeping Stress at or below its current value).

Scoring rule (per microworld):

* S6,m=1 if the student selects option C

* S6,m=0 otherwise

Range: 0‚Äì1

*Strategy Use (Process Indicators; Items 7‚Äì8)*

Strategy-use items are scored automatically based on log data from the exploration phase (not from self-report). Each item is a binary indicator of the presence vs. absence of a specific strategic pattern.

Item 7: VOTAT Strategy Indicator  
This item captures whether the student systematically varies only one input at a time (VOTAT: vary-one-thing-at-a-time) during exploration.

A trial is coded as a valid VOTAT trial if:

* Exactly one of the three inputs changes relative to the previous trial, and

* The other two inputs remain unchanged.

Scoring rule (per microworld):

* S7,m=1 if the student produces at least three valid VOTAT trials during exploration

* S7,m=0 if they produce fewer than three valid VOTAT trials

Range: 0‚Äì1

Item 8 (Systematic Exploration Sequence):

This item captures whether the student organises their exploration into an ordered sequence in which each input is tested individually at least once.

A student‚Äôs exploration sequence in microworld m is classified as systematic if there exists at least one run of three consecutive valid single-input trials such that:

* Each of the three trials changes a different input (i.e., across the three trials, all three inputs are each tested once), and

* No trial within this run changes more than one input.

Example of a systematic run:

Study Hours only ‚Üí Group Work only ‚Üí Self-Care only (order may vary, but all three inputs are covered exactly once).

If no such run is present (e.g., repeated testing of the same input, mixed multi-input changes, or scattered single-input trials without a complete three-input sequence), the sequence is classified as random/unsystematic.

Scoring rule (per microworld):

* S8,m=1 if the exploration sequence is classified as systematic

* S8,m=0 otherwise

Range: 0‚Äì1

**Dimension-level scores per microworld**

For each microworld m, we define dimension scores by summing the corresponding item scores.

Knowledge Acquisition score per microworld

KAm=S1,m+S2,m+S3,m

Range depends on L: 0‚â§KAm‚â§3L

Knowledge Application score per microworld

KAppm=S4,m+S5,m+S6,m

Considering S4,m‚àà{0,1,2} and S5,m,S6,m‚àà{0,1}, the range: 0‚â§KAppm‚â§4

Strategy Use score per microworld

SUm=S7,m+S8,m

Considering S7,m,S8,m‚àà{0,1}, the range: 0‚â§SUm‚â§2

**Aggregation across microworlds**

Because the three microworlds are designed as parallel forms with the same structure and item types but different parameters, dimension scores are aggregated across microworlds to obtain more reliable indicators.

Knowledge Acquisition (overall)

KAtotal=m=13KAm

Range: 0‚â§KAtotal‚â§9L

Knowledge Application (overall)

KApptotal=m=13KAppm

Each KAppm‚àà\[0,4\]

Range: 0‚â§KApptotal‚â§12

Strategy Use (overall)

SUtotal=m=13SUm

Each SUm‚àà\[0,2\]

Range: 0‚â§SUtotal‚â§6

**Score Interpretation and Feedback**

Results are reported primarily at the dimension level rather than as a single total score. For each student, three composite scores are computed across the three microworlds: Knowledge Acquisition, Knowledge Application, and Strategy Use. Following pilot testing, these scores will be transformed into norm-referenced indices (e.g., percentile ranks) and grouped into qualitative performance categories such as Emerging, Developing, Proficient, and Advanced, in line with reporting practices that use proficiency levels and descriptive bands in large-scale CPS assessments such as PISA 2012\. Individual feedback to students consists of a simple three-bar CPS profile plus brief narrative interpretations for each dimension (e.g., strong structural understanding but less efficient control or limited use of systematic exploration strategies). Item-level scores are not reported to students; the assessment is explicitly framed as diagnostic and formative, which mirrors recommendations from MicroDYN- and COMPRO-based CPS research that emphasize profiles of abilities over single high-stakes scores (Krieger et al, 2021). 

Instructors and programme coordinators receive aggregated, group-level feedback (e.g., mean scores and distributions per dimension, proportions of students in each qualitative band) to identify common weaknesses and monitor the effects of instructional interventions on CPS. This approach is consistent with recent work showing that combining outcome scores with process indicators (such as VOTAT use and exploration patterns) yields rich latent CPS profiles and supports more nuanced educational decision-making (Herrmann et al, 2023). All reports include a short interpretive note stressing that the instrument captures CPS components in MicroDYN-style microworlds‚Äîstructural knowledge acquisition, short-term control, and observable exploration strategies‚Äîand does not measure other aspects such as motivation, emotional regulation, creativity, or collaborative problem solving. Consequently, results are not intended for grading or selection, but to support reflection, teaching, and research on complex problem solving in higher education (Herde et al, 2016).

**4\. Administration Procedures**

**Testing Environment**

The assessment is administered digitally using a computer-based MicroDYN-style interface with slider controls for manipulating input variables. Research using MicroDYN tasks shows that local delivery ensures smooth interaction and prevents latency-related performance distortions (Krieger et al., 2021, p. 80). Therefore, the system runs offline or locally on standard laptops or PCs to guarantee real-time responsiveness of slider movements and output updates. A mouse or high-quality trackpad is required for precise control, and a minimum screen resolution of 1024√ó768 is recommended to display the full interface without scrolling (OECD, 2017; Krieger et al., 2021).

The administration environment must operate in a secure mode, preventing access to other applications or the internet during testing. COMPRO and other MicroDYN assessments typically use restricted-access environments to ensure test security and protect log-file integrity (Greiff & W√ºstenberg, 2015). All responses and log events are stored locally and encrypted immediately after recording to maintain data protection.

**Instructions to Test Takers**

Instructions emphasize interactive exploration and self-directed manipulation of the system. In MicroDYN-based tasks, participants are asked to determine relations among inputs and outputs by systematically adjusting sliders and observing the resulting system changes (Krieger et al., 2021, p. 82). The standard instruction format follows COMPRO conventions:

 ‚ÄúYou will see a panel with variables that you can change. Your goal is to understand how the inputs influence the outputs. Try different adjustments and observe the effects. Use this information to answer the questions or reach the target values.‚Äù

A brief tutorial or warm-up task is recommended to familiarize participants with slider manipulation and the two-phase structure (knowledge acquisition followed by knowledge application), ensuring that usability issues do not confound performance (Krieger et al., 2021; Greiff & W√ºstenberg, 2015).

**Administration Responsibilities and Security**

The test administrator is responsible for ensuring secure delivery, monitoring adherence to instructions, and preventing access to other applications during the session. Administrators also verify that the hardware meets minimum interface requirements and that students complete the test individually without collaboration, consistent with guidelines for computer-based CPS tasks in research settings (Krieger et al., 2021).

**Timing**

Although individual MicroDYN tasks typically allow 3 minutes for knowledge acquisition and an additional phase for knowledge application (Krieger et al., 2021, p. 82), the full assessment session is capped at 60 minutes. A visible progress bar helps participants manage time across tasks.

**Data Capture (Log Files)**

MicroDYN assessments collect detailed event-level log data. Every interaction‚Äîsuch as slider adjustments, clicks, trial transitions, and causal-diagram entries‚Äîis timestamped and stored. (Krieger et al., 2021, pp. 82‚Äì83). In this instrument, all log events are stored in JSON format, with each action timestamped at the millisecond level. Logged event types include: START\_ITEM, MOVE\_SLIDER, CLICK\_APPLY, CLICK\_NEXT, and END\_ITEM. These logs allow extraction of key process indicators, such as:

* exploration sequences,

* use of VOTAT strategies,

* overshooting or inefficient control behavior,

* number and timing of actions.

This structure is consistent with other CPS instruments like COMPRO, where log-file analysis is essential for measuring cognitive and strategic processes (Greiff & W√ºstenberg, 2015; Stadler et al., 2019).

**Data Storage and Protection**

All log files and performance data are stored in encrypted format immediately after capture, following standards used in computer-based CPS research (Greiff & W√ºstenberg, 2015). Only authorized researchers have access to the dataset, and files are stored on secure institutional servers compliant with research ethics requirements.

**5\. Planned Psychometric Model for Instrument Scoring**

The planned scoring model follows psychometric principles demonstrated in MicroDYN research, where knowledge acquisition, knowledge application, and process indicators are treated as separate but related dimensions of CPS performance. Herrmann et al. (2023) showed that MicroDYN data are well-suited for latent-variable modeling because exploration and control behaviors produce structured, analyzable performance indicators. Their study demonstrated that scoring models incorporating both accuracy outcomes and process data (e.g., trial sequences, VOTAT use, and efficiency patterns) yield stable factor structures and improve the ability to detect individual differences in CPS. Based on these findings, our scoring approach will combine dichotomous or partial-credit scores for performance outcomes with coded logfile indicators of exploration strategies. This allows the model to capture both final task success and the cognitive processes underlying it, consistent with current best practices in CPS measurement. The final psychometric analysis will evaluate model fit, item functioning, and dimensionality following the methodological recommendations outlined by Herrmann et al. (2023).

# References {#references}

1. Chernikova, O., Heitzmann, N., Stadler, M., Holzberger, D., Seidel, T., & Fischer, F. (2020). Simulation-based learning in higher education: A meta-analysis. Review of Educational Research, 90(4), 499‚Äì541. https://doi.org/10.3102/0034654320933544  
2. Greiff, S., & Funke, J. (2009). Measuring complex problem solving: The MicroDYN approach. Psychological Test and Assessment Modeling, 51(1), 28‚Äì52. https://doi.org/10.11588/heidok.00015502  
3. Kunze, T., Stadler, M., & Greiff, S. (2018). A look at complex problem solving in the 21st century (Education: Future Frontiers Occasional Paper Series, No. 11). NSW Department of Education, Centre for Education Statistics and Evaluation. https://education.nsw.gov.au/teaching-and-learning/education-for-a-changing-world/learning-for-the-future/a-look-at-complex-problem-solving-in-the-21st-century  
4. Moln√°r, G., & Csap√≥, B. (2018). The efficacy and development of students‚Äô problem-solving strategies during an interactive CPS assessment. Frontiers in Psychology, 9, 302\. https://doi.org/10.3389/fpsyg.2018.00302  
5. Schweizer, F., W√ºstenberg, S., & Greiff, S. (2013). Validity of the MicroDYN approach: Complex problem solving predicts school grades beyond working memory capacity. Learning and Individual Differences, 24, 42‚Äì52. https://doi.org/10.1016/j.lindif.2012.12.011  
6. Stadler, M., Fischer, F., & Greiff, S. (2019). Successful and unsuccessful problem solvers: The role of strategy use. Frontiers in Psychology, 10, 777\. https://doi.org/10.3389/fpsyg.2019.00777  
7. Tushar, H., & Sooraksa, N. (2023). Global employability skills in the 21st-century workplace: A semi-systematic literature review. Heliyon, 9(11), e21023. https://doi.org/10.1016/j.heliyon.2023.e21023  
8. Zhang, C., Wang, P., Zeng, X., & Wang, X. (2025). Developing students‚Äô problem-solving skills through interdisciplinary thematic learning. Frontiers in Psychology, 16, 1447089\. https://doi.org/10.3389/fpsyg.2025.1447089  
9. Funke, J. (2010). Complex problem solving: A case for complex cognition? Cognitive Processing, 11(2), 133‚Äì142. https://doi.org/10.1007/s10339-009-0345-0  
10. Funke, J. (2010). Complex problem solving: A case for complex cognition? Cognitive Processing, 11(2), 133‚Äì142. https://doi.org/10.1007/s10339-009-0345-0  
11. Greiff, S., W√ºstenberg, S., & Funke, J. (2012). Dynamic problem solving: A new assessment perspective. Applied Psychological Measurement, 36(3), 189‚Äì213. https://doi.org/10.1177/0146621612439620  
12. Greiff, S., W√ºstenberg, S., Moln√°r, G., Fischer, A., Funke, J., & Csap√≥, B. (2013). Students‚Äô performance in complex problem solving: An investigation of task characteristics and individual differences. Learning and Instruction, 23, 16‚Äì28.  
13. OECD. (2014). PISA 2012 results: Creative problem solving (Volume V). OECD Publishing. https://doi.org/10.1787/9789264208070-en  
14. Alrababah, S. A., Wu, H., & Moln√°r, G. (2024). A pilot study for measuring complex problem-solving in Jordan: Feasibility, construct validity, and behavior pattern analyses. SAGE Open, 14(2), 1‚Äì13. https://doi.org/10.1177/21582440241249884  
15. Greiff, S., W√ºstenberg, S., Holt, D. V., Goldhammer, F., & Funke, J. (2013). Computer-based assessment of complex problem solving: Concept, implementation, and application. Journal of Educational Psychology, 105(2), 364‚Äì379. https://doi.org/10.1037/a0031859  
16. Herrmann, W., Beckmann, J. F., & Kretzschmar, A. (2023). The role of learning in complex problem solving using MicroDYN. Intelligence, 100, 101773\. https://doi.org/10.1016/j.intell.2023.101773  
17. Wu, H., & Moln√°r, G. (2021). Logfile analyses of successful and unsuccessful strategy use in complex problem-solving: A cross-national comparison study. European Journal of Psychology of Education, 36, 1009‚Äì1032. https://doi.org/10.1007/s10212-020-00516-y  
18. Greiff, S., Holt, D. V., & Funke, J. (2013). Perspectives on problem solving in educational assessment: Analytical, interactive, and collaborative problem solving. The Journal of Problem Solving, 5(2), 71‚Äì91. https://doi.org/10.7771/1932-6246.1153  
19. Fischer, A., Greiff, S., & Funke, J. (2017). The history of complex problem solving. In B. Csap√≥ & J. Funke (Eds.), The nature of problem solving: Using research to inspire 21st century learning (pp. 107‚Äì122). OECD Publishing. https://doi.org/10.1787/9789264273955-9-en  
20. Schoppek, W., & Fischer, A. (2015). Complex problem solving‚ÄîSingle ability or complex phenomenon? Frontiers in Psychology, 6, 1669\. https://doi.org/10.3389/fpsyg.2015.01669  
21. Herrmann, W., Beckmann, J. F., & Kretzschmar, A. (2023). The role of learning in complex problem solving using MicroDYN. Intelligence, 100, 101773\. https://doi.org/10.1016/j.intell.2023.101773

[^1]:  These item types were selected because they correspond directly to the observable behaviours associated with identifying causal relations in MicroDYN tasks. Other knowledge-acquisition processes described in broader problem-solving literature cannot be represented by the item formats below.

[^2]:  These items were chosen because the MicroDYN control phase allows direct observation of how students apply discovered relations to reach target states. The item types reflect short-term system control but not broader decision-making processes.

[^3]:  These item types capture only the exploration strategies observable through trial data in MicroDYN and do not represent the full spectrum of strategic or metacognitive processes in general problem-solving theory.